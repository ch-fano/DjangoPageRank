{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T14:58:20.819167Z",
     "start_time": "2024-08-31T14:58:20.808617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.chdir('..')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".git\n",
      ".idea\n",
      ".venv\n",
      "db.sqlite3\n",
      "DjangoRecipesRank\n",
      "docs\n",
      "manage.py\n",
      "README.md\n",
      "requirements.txt\n",
      "SearchEngine\n",
      "static\n",
      "templates\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:03:00.646933Z",
     "start_time": "2024-08-31T15:02:47.056414Z"
    }
   },
   "source": [
    "import json\n",
    "from whoosh.scoring import BM25F\n",
    "from SearchEngine.word2vec.doc2vec_model import Doc2VecModel\n",
    "from SearchEngine.sentiment.sentiment_model import SentimentModelWA, SentimentModelARWA\n",
    "from SearchEngine.sentiment.reviews import ReviewsIndex\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "with open('./SearchEngine/benchmark/queries.json') as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# Models that need to be tested. \n",
    "models = [\n",
    "\t(BM25F(), \"BM25F\"),\n",
    " \t(Doc2VecModel(), \"Doc2Vec\"),\n",
    "\t(SentimentModelWA(ReviewsIndex()), \"Sentiment Weighted Average\"),\n",
    "\t(SentimentModelARWA(ReviewsIndex()), \"Sentiment Weighted Average Reviews\")\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing all the available queries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:03:13.123608Z",
     "start_time": "2024-08-31T15:03:13.090020Z"
    }
   },
   "source": [
    "indexes, uin = [i for i in range(len(queries))], [k[\"UIN\"] for k in queries]\n",
    "print('ID \\t UIN')\n",
    "print('\\n'.join([f\"{x[0]} \\t {x[1]}\" for x in list(zip(indexes, uin))]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID \t UIN\n",
      "0 \t I need an apartment near st james park\n",
      "1 \t enjoyable double room where i could stay with my dog.\n",
      "2 \t I'm a student looking for a budget-friendly hostel in Kensington or Westminster. I don't care about having all kind of amenities.\n",
      "3 \t I'm looking for a luxurious apartment in Kensington for a family of four, with two bathrooms and a beautiful view.\n",
      "4 \t I need a cheap place in Camden that allows dogs, ideally with a minimum rating of 3 stars.\n",
      "5 \t My partner and I are looking for a romantic getaway. We want an apartment rated at least 4 stars.\n",
      "6 \t I need a budget-friendly room in Hackney, with access to a shared bathroom and a rating of at least 2 stars.\n",
      "7 \t We are a group of friends looking for a large apartment in Tower Hamlets with at least 3 beds and 2 bathrooms. Budget is up to â‚¬500 per night.\n",
      "8 \t I'm searching a creepy and old place for an Halloween party. Looking for a budget-friendly place, rated at least 3 stars. Max budget is â‚¬80.\n",
      "9 \t I need a luxurious studio in London City for a work trip, rated at least 4 stars\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query selection occurs by assigning a value to the variable ***examined_q*** within the available range shown above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:08:13.322217Z",
     "start_time": "2024-08-31T15:08:13.311663Z"
    }
   },
   "source": [
    "examined_q = None\n",
    "try:\n",
    "    examined_q = 5\n",
    "    print(\"User Information Need: \" + queries[examined_q][\"UIN\"])\n",
    "except IndexError as e:\n",
    "    print(\"index not valid\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Information Need: My partner and I are looking for a romantic getaway. We want an apartment rated at least 4 stars.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, an object Benchmark is created.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:08:31.141939Z",
     "start_time": "2024-08-31T15:08:30.333692Z"
    }
   },
   "source": [
    "from SearchEngine.benchmark.benchmark_functions import Benchmark\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Suppress the warning \n",
    "\n",
    "b = Benchmark(queries[examined_q])\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Evaluation Phase*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision at Standard Recall Levels for query Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Natural Recall Values and Precision Values, Precision at Standard Recall Levels is Computed using \n",
    "\n",
    "$P(r_j)=\\text{max}_{r_j\\le r\\le r_{j+1}}P(r)$\n",
    "\n",
    "Standard Measures is obtained by considering 11 recall points where precision is measured: 0%, 10%, 20%, … , 100%. \n",
    "\n",
    "\n",
    "The curve that sits higher or remains more stable at higher recall levels usually indicates a better system at retrieving relevant documents with high precision."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:11:05.618148Z",
     "start_time": "2024-08-31T15:11:05.324411Z"
    }
   },
   "source": [
    "# define axes' names\n",
    "axes = [\"recall\", \"precision\"]\n",
    "\n",
    "# create a dataframe for Seaborn\n",
    "df = pd.DataFrame()\n",
    "for model, model_name in models:\n",
    "    result = b.get_results(20, model)\n",
    "    # get precision at standard recall values over list of result\n",
    "    SRLValues = b.get_srl_values(\n",
    "        b.get_precision_values(result),\n",
    "        b.get_recall_values(result)\n",
    "    )\n",
    "    \n",
    "    # tmp dataframe concatenated to the main one\n",
    "    dfB = pd.DataFrame(SRLValues, columns = axes)\n",
    "    dfB[\"Version\"] = f'{model_name}'\n",
    "    \n",
    "    df = pd.concat([df, dfB])\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "# plot the line graph\n",
    "pltP = sns.lineplot(data = df, x = 'recall', y = 'precision', marker='o', markersize=4, hue=\"Version\", palette=\"colorblind\")\n",
    "pltP.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltP.set_xlim([-0.1, 1.1]);\n",
    "pltP.set_ylim([-0.1, 1.1]);\n",
    "\n",
    "print(b)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, model_name \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[1;32m----> 7\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# get precision at standard recall values over list of result\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     SRLValues \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mget_srl_values(\n\u001B[0;32m     10\u001B[0m         b\u001B[38;5;241m.\u001B[39mget_precision_values(result),\n\u001B[0;32m     11\u001B[0m         b\u001B[38;5;241m.\u001B[39mget_recall_values(result)\n\u001B[0;32m     12\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\DjangoRecipesRank\\SearchEngine\\benchmark\\benchmark_functions.py:17\u001B[0m, in \u001B[0;36mBenchmark.get_results\u001B[1;34m(self, n_result, model, verbose)\u001B[0m\n\u001B[0;32m     13\u001B[0m my_model \u001B[38;5;241m=\u001B[39m IRModel(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, model)\n\u001B[0;32m     14\u001B[0m result \u001B[38;5;241m=\u001B[39m my_model\u001B[38;5;241m.\u001B[39msearch(query\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m], res_limit\u001B[38;5;241m=\u001B[39mn_result, sentiments\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentiments\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     15\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[1;32m---> 17\u001B[0m results \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mid\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m()]\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRelevant documents: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevant_documents\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision At Standard Recall Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision at standard recall levels is calculated as in the previous example, but this time not for a single query but for all of them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "IAPatSRL = {}\n",
    "\n",
    "for q in queries:\n",
    "    tmpB = Benchmark(q)\n",
    "    for model, model_name in models:\n",
    "        result = tmpB.get_results(20, model)\n",
    "        SRLValues = tmpB.get_srl_values(\n",
    "            tmpB.get_precision_values(result),\n",
    "            tmpB.get_recall_values(result)\n",
    "            )\n",
    "        IAPatSRL.setdefault(model_name, []).append(\n",
    "\t\t\tSRLValues\n",
    "\t\t)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the average precision is calculated for each recall level."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from functools import reduce\n",
    "import matplotlib as plt\n",
    "meansDict = {}\n",
    "\n",
    "for k,v in IAPatSRL.items():\n",
    "    # Trasporre la lista di liste per ottenere gli elementi corrispondenti\n",
    "    transposed = list(zip(*v))\n",
    "\n",
    "    # Calcolare la media dei secondi elementi delle tuple utilizzando reduce\n",
    "    means = []\n",
    "    for tuples in transposed:\n",
    "        mean = reduce(lambda acc, t: acc + t[1], tuples, 0) / len(tuples)\n",
    "        means.append((tuples[0][0], round(mean,2)))\n",
    "    \n",
    "    meansDict[k] = means\n",
    "\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_list = []\n",
    "\n",
    "for key, value in meansDict.items():\n",
    "    for x, y in value:\n",
    "        df_list.append({'Version': key, 'Recall': x, 'Precision': y})\n",
    "\n",
    "df = pd.DataFrame(df_list)\n",
    "\n",
    "# Plotting with Seaborn\n",
    "pltIAPatSRL = sns.lineplot(data=df, x='Recall', y='Precision', hue='Version', marker='o', markersize=4, palette=\"colorblind\")\n",
    "pltIAPatSRL.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left');\n",
    "\n",
    "pltIAPatSRL.set_xlim([-0.1, 1.1]);\n",
    "pltIAPatSRL.set_ylim([-0.1, 1.1]);\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolated Average Precision (IAP) at Standard Recall Levels for query Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolated Average Precision for each query is computed using \n",
    "\n",
    "$\\sum_{r=0}^{n} \\frac{P_q(r)}{n}$\n",
    "\n",
    "where $P_q(r)$ is the interpolated precision at the level $r$, for n+1 standard recall levels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T15:14:16.588753Z",
     "start_time": "2024-08-31T15:14:16.433283Z"
    }
   },
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "import textwrap\n",
    "\n",
    "versions = [] \n",
    "AvPr_values=[]\n",
    "\n",
    "for model, model_name in models:\n",
    "    result = b.get_results(20, model)\n",
    "    SRLValues = b.get_srl_values(\n",
    "        b.get_precision_values(result),\n",
    "        b.get_recall_values(result)\n",
    "    )\n",
    "    \n",
    "    AvPr_values.append(b.get_i_ap_avg_precision(SRLValues))\n",
    "    versions.append(textwrap.fill(model_name, width=10,\n",
    "                    break_long_words=True))\n",
    "    \n",
    "# plot the average precisions\n",
    "# apply the default theme\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "# create a dataframe for Seaborn\n",
    "df = pd.DataFrame({\"Search-engine version\": versions, \"IAP at SRL\": AvPr_values})\n",
    "\n",
    "# plot the bar graph\n",
    "pltAvPr = sns.barplot(data = df, x = \"Search-engine version\", y = 'IAP at SRL',palette=\"colorblind\")\n",
    "\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltAvPr.set_ylim([0.0, max(AvPr_values)+0.20]); # set y-axis    \n",
    "pltAvPr.yaxis.set_major_locator(MultipleLocator(0.05))\n",
    "\n",
    "print(b)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m AvPr_values\u001B[38;5;241m=\u001B[39m[]\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, model_name \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[1;32m----> 8\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_results\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     SRLValues \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mget_srl_values(\n\u001B[0;32m     10\u001B[0m         b\u001B[38;5;241m.\u001B[39mgetPrecisionValues(result),\n\u001B[0;32m     11\u001B[0m         b\u001B[38;5;241m.\u001B[39mgetRecallValues(result)\n\u001B[0;32m     12\u001B[0m     )\n\u001B[0;32m     14\u001B[0m     AvPr_values\u001B[38;5;241m.\u001B[39mappend(b\u001B[38;5;241m.\u001B[39mgetIapAvgPrecision(SRLValues))\n",
      "File \u001B[1;32m~\\PycharmProjects\\DjangoRecipesRank\\SearchEngine\\benchmark\\benchmark_functions.py:17\u001B[0m, in \u001B[0;36mBenchmark.get_results\u001B[1;34m(self, n_result, model, verbose)\u001B[0m\n\u001B[0;32m     13\u001B[0m my_model \u001B[38;5;241m=\u001B[39m IRModel(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, model)\n\u001B[0;32m     14\u001B[0m result \u001B[38;5;241m=\u001B[39m my_model\u001B[38;5;241m.\u001B[39msearch(query\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m], res_limit\u001B[38;5;241m=\u001B[39mn_result, sentiments\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentiments\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     15\u001B[0m                             verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[1;32m---> 17\u001B[0m results \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mid\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m()]\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRelevant documents: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevant_documents\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Precision for query Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how many relevant documents I have returned compared to the ideal case in which in the first $n$ documents are all relevants. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "versions = [] \n",
    "RP_values=[]\n",
    "\n",
    "for model, model_name in models:\n",
    "    result = b.get_results(20, model)\n",
    "    \n",
    "    RP_values.append(b.get_r_precision(result))\n",
    "    \n",
    "    versions.append(textwrap.fill(model_name, width=10,\n",
    "                break_long_words=True))\n",
    "\n",
    "# plot the average precisi+ons\n",
    "# apply the default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create a dataframe for Seaborn\n",
    "df = pd.DataFrame({\"Search-engine version\": versions, 'Precision@R': RP_values})\n",
    "\n",
    "# plot the bar graph\n",
    "pltRP = sns.barplot(data = df, x = \"Search-engine version\", y = 'Precision@R', palette=\"colorblind\")\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltRP.set_ylim([0.0, max(AvPr_values)+0.20]); # set y-axis    \n",
    "pltRP.yaxis.set_major_locator(MultipleLocator(0.05))\n",
    "\n",
    "print(b)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Precision Comparison between two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models selection occurs by assigning a value to the two dedicated variables from the available models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "'''\n",
    "Available models: \n",
    "- \"BM25F\"\n",
    "- \"Doc2Vec\"\n",
    "- \"Sentiment Weighted Average\"\n",
    "- \"Sentiment Weighted Average Reviews\"\n",
    "'''\n",
    "\n",
    "model1 = \"BM25F\"\n",
    "model2 = \"Doc2Vec\"\n",
    "\n",
    "for model, model_name in models:\n",
    "    if model1 == model_name:\n",
    "        model1 = (model, model_name)\n",
    "    if model2 == model_name:\n",
    "        model2 =(model, model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the R-Precision for both models and for each query. Subsequently, save the difference between them. \n",
    "\n",
    "This is the formula used:\n",
    "\n",
    "$RP_{A/B}(i) =RP_{A}(i) -RP_{B}(i) $\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "RP_comparison = []\n",
    "for q in queries:\n",
    "    tmpB = Benchmark(q)\n",
    "    model1Res = tmpB.getResults(20, model1[0])\n",
    "    model2Res = tmpB.getResults(20, model2[0])\n",
    "    \n",
    "    RP_comparison.append(\n",
    "        tmpB.get_r_precision(model1Res) - tmpB.get_r_precision(model2Res)\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A positive value indicates that *model A* has a higher *R_Precision* than *B* for that specific query, while a negative value indicates the opposite. This comparison provides an indication of which model is more effective in correctly ranking the primes $n$ relevant documents.\n",
    "\n",
    "\n",
    "Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({\n",
    "    'Queries': range(0, len(RP_comparison)),\n",
    "    'R-Precision A/B': RP_comparison\n",
    "})\n",
    "\n",
    "pltRP_comp = sns.barplot(x='Queries', y='R-Precision A/B', data=df, color='skyblue')\n",
    "pltRP_comp.set_title(f'R-Precision Comparison between {model1[1]} (A) and {model2[1]} (B)');\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, get Non-Interpolated Average Precision for each query using $ \\sum_{r=0}^{n} \\frac{P_q(r/|R_q|)}{|R_q|} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, it is an average where we sum only the precision (precision at natural level of recall) corresponding to the relevant documents since the recall does not change between a relevant document and a non-relevant document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure favors systems that return documents relevant to the query q quickly, i.e. in the top positions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "NIAP_dict = {}\n",
    "\n",
    "for q in queries:\n",
    "    \n",
    "    tmpB = Benchmark(q)\n",
    "    \n",
    "    for model, model_name in models:\n",
    "        result = tmpB.get_results(20, model)\n",
    "        NIAP_dict.setdefault(model_name, []).append(tmpB.get_ni_ap_avg_precision(\n",
    "            tmpB.get_precision_values(result),\n",
    "            tmpB.get_recall_values(result),\n",
    "        ))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, compute Mean Average Precision for every model which is just an average between all the NIAP. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MAP_list = []\n",
    "versions = []\n",
    "\n",
    "for model_name, p_list in NIAP_dict.items():\n",
    "    MAP_list.append(round(sum(p_list)/len(p_list),2) if len(p_list) != 0 else 0)\n",
    "            \n",
    "    versions.append(textwrap.fill(model_name, width=10,\n",
    "                break_long_words=True))\n",
    "\n",
    "# apply the default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# create a dataframe for Seaborn\n",
    "df = pd.DataFrame({\"Search-engine version\": versions, 'MAP': MAP_list})\n",
    "\n",
    "# plot the bar graph\n",
    "pltRP = sns.barplot(data = df, x = \"Search-engine version\", y = 'MAP', palette=\"colorblind\")\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltRP.set_ylim([0.0, max(AvPr_values)+0.20]); # set y-axis    \n",
    "pltRP.yaxis.set_major_locator(MultipleLocator(0.05))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Measure & E-Measure for query Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Measure (or harmonic mean) is higher when both reacll and precision are high, there must be \"harmony\" between the two values. \n",
    "\n",
    "$$\n",
    "\\text{F-measure}=\\frac{2}{\\frac{1}{prec.}+\\frac{1}{recall}}=\\frac{2*prec.*recall}{prec.+recall}\n",
    "$$\n",
    "\n",
    "This metric combines precision and recall in order to obtain an unique value.\n",
    "\n",
    "E-measure is a variant of the harmonic mean which allows us to emphasize the value of recall or precision based on what we are interested in.\n",
    "\n",
    "$$\n",
    "\\text{E-measure}=1-\\frac{1+b^2}{\\frac{b^2}{recall}+\\frac{1}{prec.}}\n",
    "$$\n",
    "\n",
    "In particular: \n",
    "- $b=1 \\rightarrow 1-\\text{F-Measure}$ \n",
    "- $b>1$ emphasize precision\n",
    "- $b<1$ emphasize recall\n",
    "\n",
    "*Precision or Recall?*\n",
    "\n",
    "- High Recall: relevant documents, but with too many unrelevant documents. \n",
    "- High Precision: few results but with an greater probability of being relevant. \n",
    "\n",
    "It's possible to customize *b* value."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "b_recall = 0.5          # Default value for emphasizing recall\n",
    "b_precision = 1.5       # Default value for emphasizing precision\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Search-engine version\"])\n",
    "for model, model_name in models:    \n",
    "    result = b.get_results(20, model)\n",
    "\n",
    "    data = {\n",
    "    'F-Measure': [b.get_f_measure(result)],\n",
    "    'E-Measure Recall': [b.get_e_measure(result, b_recall)],\n",
    "    'E-Measure Precision': [b.get_e_measure(result, b_precision)]\n",
    "    }\n",
    "\n",
    "    tmpDf = pd.DataFrame(data)\n",
    "    tmpDf[\"Search-engine version\"] = textwrap.fill(model_name, width=10,\n",
    "                break_long_words=True)\n",
    "    \n",
    "    df = pd.concat([df, tmpDf])\n",
    "    \n",
    "\n",
    "# apply the default theme\n",
    "sns.set_theme()\n",
    "\n",
    "df_long = df.melt(id_vars='Search-engine version', var_name='Metric', value_name='Measure\\'s Value')\n",
    "\n",
    "# Crea il barplot\n",
    "pltMeasures = sns.barplot(x='Search-engine version', y='Measure\\'s Value', hue='Metric', data=df_long, palette='icefire')\n",
    "pltMeasures.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# set fixed axes, the semicolon suppress the output\n",
    "pltMeasures.set_ylim([0.0, df.iloc[:, -3:].max().max()+0.1]); # set y-axis   \n",
    "\n",
    "infolabel = f'E-Measure b value:\\n- b for emphasizing recall: {b_recall} \\n- b for emphasizing precision: {b_precision}'\n",
    "pltMeasures.text(x=pltMeasures.get_xlim()[1] + 0.3, y=(pltMeasures.get_ylim()[0] + pltMeasures.get_ylim()[1]) / 2, s=infolabel, fontsize=12, color='black', ha='left', va='center', rotation=0);\n",
    "\n",
    "print(b)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
